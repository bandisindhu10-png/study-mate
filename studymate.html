<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
</head>
<body>
 """
StudyMate - AI-Powered Academic Assistant
A comprehensive study tool for interactive learning with PDF documents
Powered by IBM Granite 3.3-2B-Instruct
"""

import streamlit as st
import fitz  # PyMuPDF
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed
import os
from typing import List, Dict, Optional
import tempfile
import time
import traceback

# ============================================================================
# PAGE CONFIGURATION
# ============================================================================

st.set_page_config(
    page_title="StudyMate - AI Study Assistant",
    page_icon="üìö",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============================================================================
# CUSTOM CSS STYLING
# ============================================================================

st.markdown("""
    <style>
    /* Main Headers */
    .main-header {
        font-size: 3.5rem;
        font-weight: 700;
        text-align: center;
        background: linear-gradient(120deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        margin-bottom: 0.5rem;
        padding: 1rem 0;
    }
    
    .sub-header {
        text-align: center;
        color: #666;
        font-size: 1.2rem;
        margin-bottom: 2rem;
        font-weight: 400;
    }
    
    /* Question Box */
    .question-box {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1.5rem;
        border-radius: 12px;
        margin: 1.5rem 0;
        font-weight: 500;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    
    /* Answer Box */
    .answer-box {
        background: linear-gradient(to right, #f0f8ff 0%, #e8f4f8 100%);
        padding: 2rem;
        border-radius: 12px;
        border-left: 6px solid #667eea;
        margin: 1.5rem 0;
        line-height: 1.8;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
    }
    
    /* Source Box */
    .source-box {
        background: linear-gradient(to right, #fff9f0 0%, #fff4e6 100%);
        padding: 1.2rem;
        border-radius: 10px;
        border-left: 5px solid #FFA500;
        margin: 1rem 0;
        font-size: 0.95rem;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
    }
    
    /* Info Boxes */
    .info-box {
        background-color: #e3f2fd;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #2196f3;
        margin: 1rem 0;
    }
    
    .success-box {
        background-color: #e8f5e9;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #4caf50;
        margin: 1rem 0;
    }
    
    .warning-box {
        background-color: #fff3e0;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #ff9800;
        margin: 1rem 0;
    }
    
    .error-box {
        background-color: #ffebee;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #f44336;
        margin: 1rem 0;
    }
    
    /* Stat Cards */
    .stat-card {
        background: white;
        padding: 1.5rem;
        border-radius: 10px;
        box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        text-align: center;
        transition: transform 0.2s;
    }
    
    .stat-card:hover {
        transform: translateY(-5px);
        box-shadow: 0 4px 12px rgba(0,0,0,0.15);
    }
    
    /* Buttons */
    .stButton>button {
        border-radius: 8px;
        font-weight: 600;
        transition: all 0.3s;
    }
    
    /* Custom Scrollbar */
    ::-webkit-scrollbar {
        width: 10px;
    }
    
    ::-webkit-scrollbar-track {
        background: #f1f1f1;
    }
    
    ::-webkit-scrollbar-thumb {
        background: #888;
        border-radius: 5px;
    }
    
    ::-webkit-scrollbar-thumb:hover {
        background: #555;
    }
    </style>
""", unsafe_allow_html=True)

# ============================================================================
# STUDYMATE CLASS - CORE FUNCTIONALITY
# ============================================================================

class StudyMate:
    """
    Main StudyMate class handling:
    - PDF text extraction
    - Semantic search with FAISS
    - Answer generation with IBM Granite
    """
    
    def __init__(self):
        """Initialize StudyMate with default configurations"""
        self.embedding_model = None
        self.llm_model = None
        self.tokenizer = None
        self.index = None
        self.chunks = []
        self.chunk_metadata = []
        
        # Auto-detect device (GPU/CPU)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
    @st.cache_resource
    def load_embedding_model(_self):
        """
        Load SentenceTransformer model for creating text embeddings
        Returns: SentenceTransformer model or None on error
        """
        try:
            with st.spinner("üîÑ Loading embedding model..."):
                model = SentenceTransformer('all-MiniLM-L6-v2')
                st.success("‚úÖ Embedding model loaded successfully!")
                return model
        except Exception as e:
            st.error(f"‚ùå Error loading embedding model: {str(e)}")
            st.error("Try: pip install --upgrade sentence-transformers")
            return None
    
    @st.cache_resource
    def load_llm_model(_self):
        """
        Load IBM Granite 3.3-2B-Instruct model for answer generation
        Uses exact implementation from provided code snippet
        Returns: (model, tokenizer) tuple or (None, None) on error
        """
        try:
            with st.spinner("ü§ñ Loading IBM Granite LLM... This may take several minutes on first run."):
                model_path = "ibm-granite/granite-3.3-2b-instruct"
                
                # Exact implementation from provided code
                model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    device_map=_self.device,
                    torch_dtype=torch.bfloat16,
                )
                
                tokenizer = AutoTokenizer.from_pretrained(model_path)
                
                st.success(f"‚úÖ IBM Granite model loaded successfully on {_self.device.upper()}!")
                return model, tokenizer
                
        except Exception as e:
            st.error(f"""
            ‚ùå **Error loading LLM model**
            
            Error: {str(e)}
            
            **Troubleshooting:**
            1. Ensure stable internet connection (downloading ~5GB)
            2. Check free disk space (~10GB required)
            3. Update transformers: `pip install --upgrade transformers accelerate`
            4. If on CPU, ensure you have enough RAM (8GB+ recommended)
            """)
            return None, None
    
    def extract_text_from_pdf(self, pdf_file, filename: str) -> List[Dict]:
        """
        Extract text chunks from PDF with metadata
        
        Args:
            pdf_file: Uploaded PDF file object
            filename: Name of the PDF file
            
        Returns:
            List of dictionaries containing text chunks and metadata
        """
        chunks_with_metadata = []
        temp_path = None
        
        try:
            # Save uploaded file to temporary location
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                tmp_file.write(pdf_file.read())
                temp_path = tmp_file.name
            
            # Open PDF with PyMuPDF
            doc = fitz.open(temp_path)
            
            # Check if PDF is empty
            if len(doc) == 0:
                st.warning(f"‚ö†Ô∏è {filename}: PDF appears to be empty or corrupted")
                return chunks_with_metadata
            
            # Extract text from each page
            for page_num in range(len(doc)):
                try:
                    page = doc[page_num]
                    text = page.get_text()
                    
                    # Skip empty pages
                    if not text.strip():
                        continue
                    
                    # Split text into paragraphs
                    paragraphs = text.split('\n\n')
                    
                    for para in paragraphs:
                        cleaned = para.strip()
                        
                        # Filter chunks by length (50-2000 chars for optimal context)
                        if 50 <= len(cleaned) <= 2000:
                            chunks_with_metadata.append({
                                'text': cleaned,
                                'source': filename,
                                'page': page_num + 1,
                                'total_pages': len(doc)
                            })
                            
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Error on page {page_num + 1} of {filename}: {str(e)}")
                    continue
            
            doc.close()
            
            # Report extraction results
            if chunks_with_metadata:
                st.success(f"‚úÖ {filename}: Extracted {len(chunks_with_metadata)} text chunks")
            else:
                st.warning(f"""
                ‚ö†Ô∏è {filename}: No text chunks extracted
                
                **Possible reasons:**
                - PDF contains only images (scanned document)
                - Text is in an unsupported format
                - PDF quality is poor
                
                **Solution:** Use text-based PDFs or apply OCR to scanned documents
                """)
            
        except Exception as e:
            st.error(f"‚ùå Error processing {filename}: {str(e)}")
            
        finally:
            # Clean up temporary file
            if temp_path and os.path.exists(temp_path):
                try:
                    os.unlink(temp_path)
                except:
                    pass
        
        return chunks_with_metadata
    
    def create_vector_store(self, chunks_with_metadata: List[Dict]) -> bool:
        """
        Create FAISS vector store for semantic search
        
        Args:
            chunks_with_metadata: List of text chunks with metadata
            
        Returns:
            True if successful, False otherwise
        """
        try:
            # Validate input
            if not chunks_with_metadata:
                st.error("‚ùå No text chunks to process. Please check your PDF files.")
                return False
            
            # Load embedding model if needed
            if not self.embedding_model:
                self.embedding_model = self.load_embedding_model()
                if not self.embedding_model:
                    return False
</body>
</html>